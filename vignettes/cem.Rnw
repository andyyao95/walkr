% !Rnw weave = knitr

\documentclass{article}
\usepackage{verbatim}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{subfig}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\MakeOuterQuote{"}

\begin{document}

\title{Coarsened Exact Matching}
\maketitle

We summarize the key components to Gary King's paper (Casual Inference without Balance Checking:
Coarsened Exact Matching) and attempt to replicate its results.

\section*{Summary of Paper}

\subsection*{Introduction}

In general, matching is a ``nonparametric method of controlling for the counfound in
infleunce of pretreatment control varaibles in observational data''. According to the authors, numerous
applications of matching methods fail because they introduce bias and model dependence. Therefore, they
aim to reduce bias and model dependence with this approach. \\

\noindent Their method of matching is called ``Coarsened Exact Matching'' (CEM). CEM works in sample and requires
no assumptions about the data generation process. CEM also guarantees that the imbalance
between the matched treated and control groups will not be larger than the ex ante user choice. More importantly,
CEM can be thought of as an easy first line of defnese in protecting users from the threats to making valid
casual inferences.

\subsection*{Preliminaries}

\begin{itemize}

  \item{$n$ is sample size, $N$ is population size, usually $n << N$}
  \item{$T_i$ -- a boolean variable for the $i^{th}$ unit, that is $1$ if treated and $0$ if untreated}
  \item{$Y_i(1)$ -- potential outcome if treated (always observed)}
  \item{$Y_i(0)$ -- potential outcome if untreated (never observed, always estimated)}
  \item{$Y_i = T_iY_i(1) + (1-T_i)Y_i(0)$ -- observed outcome variable}

\end{itemize}

The basic idea of CEM is to ``coarsen'' each variable so that substantively indistinguishable values
are grouped and assigend the same numerical value. In other words, group the similar units,
assign a single treated/untreated value to each group, match on the coarsened groups, and finally
the original values of the matched data are retained. Note that the groups may be the same or different
size depending on the problem. \\

\begin{itemize}

  \item{$T^S$ -- the set of treated units in stratum $S$}
  \item{$C^S$ -- the set of control units in stratum $S$}
  \item{$m_T^S$ -- number of treated units in stratum $S$}
  \item{$m_C^S$ -- number of control units in stratum $S$}
  \item{$m_T$ -- total number of treated units}
  \item{$c_C$ -- total number of control units}

\end{itemize}

{\bf Key:} To each matched unit $i$ in stratum $S$, CEM assigns the following weights:

$$
w_i = \left\{
        \begin{array}{ll}
            1 & \quad i \in T^S \\
            \frac{m_C}{m_T}\frac{m_T^S}{m_C^S} & \quad i \in C^S
        \end{array}
    \right.
$$

Unmatched units receive weight $w_i = 0$. We still need to understand more why weights are assigned this way. According to the
authors, CEM assigns to matching the task of eliminating all imbalances (i.e. differences between the treated and
control groups). Why though? \\

{\bf Measuring Imbalances}: They define a metric $\mathbb{L}_1$, which seem to resemble the typical $\mathbb{L}_1$ norm
(i.e. sum of absolute differences). So far, I don't think we need to delve deep into understanding this notion of ``distance'' or
``imbalance'' (it also seems highly technical and messy). \\

\noindent The most common quantities of interest are the
{\it sample} and {\it population average treatment effect on the treated} (SATT, PATT). $TE_i = Y_i(1) - Y_i(0)$ is the
treatment effect for unit $i$ (although we cannot observe it). Thus, we're interested in:

$$SATT = \frac{1}{n_T} \sum _{i \in T} TE_i$$

$$PATT = \frac{1}{N_T} \sum _{i \in T} TE_i$$

$\mathbb{E}(SATT) = PATT$ if sample is drawn randomly from the population. In plain words, the quantity of interest
is the {\bf treatment effect on the treated}. \\


\subsection*{Coarsening, Properties of CEM}

``Coarsening is almost intrinsic to the act of measurement''. The measurement of the data is not perfect and thus subject to
some degree of coarsening. Moreover, data analysts recognize that many measurements include some degree of noise, and often
voluntarily coarsen the data themselves. ``CEM involves less onerous assumptions than made by researchers who make the coarsening
permanent'', because at the end of CEM, the original (uncoarsened values) of the matched data are retained. A key piece of
intuition is that we can think of coarsening as constructing the bins for multi-dimensional histograms (of course, as it was the
case with the 1D histogram, this is subject to certain dangers). \\

Chapter 4 describes in the detail the properties of CEM. Some important ones, I believe, are:

\begin{itemize}
  \item{Meeting the congruence principle -- requiring congruence between data space and analysis space. If the matching method
        operate on spaces with different metrics, it violates this principle. This sometimes lead to less robust inferences and
        suboptimal properties.}
  \item{certain theoretical bounds on the estimation error}
  \item{computational efficiency}
\end{itemize}

It seems like chapter 4.8 hints at certain cases in which CEM also suffers from the curse of dimensionality. How does kmatching
work in this case?

\subsection*{Lalonde Data Set/Result}

The Lalonde data come from the NSWD 1986 job training program that provided training to the participants for 12-18 months
and helped them in finding a job. The goal of the program was to increase participants' earnings, and so 1979 earnings
(re78) is the key outcoem variable. The experimental data set consists of 297 treated and 425 controlled units. Essentially,
they show that their CEM algorithm is better than some of the other matching algorithms out there. Namely, with a lower
$\mathbb{L}_1$ imbalance measure, lower bias, standard deviation, etc.

\section*{Replication}

\url{http://gking.harvard.edu/files/gking/files/cem.pdf?m=1360071263} provides a nice manual for the replication of 
their result. It also has a nice overview of the CEM algorithm. \\

First, we must install their package: 

<<eval=FALSE>>=

install.packages("cem")
library(cem)
data(LeLonde)
## might have to install x11

@

``Matching is not a method of estimation; it is a way to preprocess a data set so that
estimation of SATT based on the matched data set will be less model dependent''. 

<<eval = FALSE>>=
Le <- data.frame(na.omit(LeLonde))

tr <- which(Le$treated==1)
ct <- which(Le$treated==0)
ntr <- length(tr)
nct <- length(ct)

@

We see that there are 258 treated and 392 untreated. We can compute the $\mathbb{L}_1$ imbalance measure:

<<eval = FALSE>>=

vars <- names(Le)

##todrop drop variable
imbalance(group = Le$treated, data = Le[vars], drop = c("treated", "re78"))

@


% at section 3.2...


\end{document}
